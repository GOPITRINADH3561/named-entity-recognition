{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83fc8cc3-fc4a-4d0d-9741-8513bac3c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "243ecaea-118c-405e-ac06-5a65622ef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72cbde25-67d1-417d-96cf-23f9a95f9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant entity tags\n",
    "relevant_entities = [\"PERSON\", \"NORP\", \"ORG\", \"GPE\", \"LOC\", \"DATE\", \"MONEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9730aca1-43d5-47a2-b8b4-3a0739681e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual Annotation Counts:\n",
      "      Tag  Count\n",
      "0    DATE    140\n",
      "2     GPE    200\n",
      "6     LOC     64\n",
      "3   MONEY     25\n",
      "4    NORP     96\n",
      "1     ORG    102\n",
      "5  PERSON     96\n"
     ]
    }
   ],
   "source": [
    "### PART 1: COUNT TAGS IN MANUAL JSON FILE ###\n",
    "manual_json_file = \"annotations.json\"  # Update with actual file path\n",
    "with open(manual_json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    manual_data = json.load(f)\n",
    "\n",
    "# Extract annotations from manual JSON\n",
    "annotations = manual_data.get(\"annotations\", [])\n",
    "manual_entity_counter = Counter()\n",
    "manual_entities = []  # List to store entities from manual annotations\n",
    "\n",
    "# Count manual entities and store for comparison\n",
    "for idx, (text, data) in enumerate(annotations):\n",
    "    entities = data.get(\"entities\", [])\n",
    "    for start, end, label in entities:\n",
    "        if label in relevant_entities:\n",
    "            manual_entities.append((idx, start, end, label))\n",
    "            manual_entity_counter[label] += 1\n",
    "\n",
    "# Convert manual counts to DataFrame for display\n",
    "manual_df = pd.DataFrame(manual_entity_counter.items(), columns=[\"Tag\", \"Count\"])\n",
    "manual_df = manual_df.sort_values(by=\"Tag\")\n",
    "\n",
    "print(\"\\nManual Annotation Counts:\")\n",
    "print(manual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46319768-f79b-4dcd-b2b6-88e69e81cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SpaCy Annotation Counts:\n",
      "      Tag  Count\n",
      "0    DATE    172\n",
      "2     GPE    215\n",
      "6     LOC     33\n",
      "3   MONEY     17\n",
      "4    NORP    105\n",
      "1     ORG    113\n",
      "5  PERSON     99\n"
     ]
    }
   ],
   "source": [
    "### PART 2: SPACY ANNOTATION ###\n",
    "tweets_text_file = \"Sentences.txt\"  # Update with actual file path\n",
    "with open(tweets_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()[:300]  # Take only first 300 tweets\n",
    "\n",
    "spacy_entity_counter = Counter()\n",
    "spacy_entities = []  # List to store entities detected by SpaCy\n",
    "\n",
    "# Collect SpaCy entities and count\n",
    "for idx, sentence in enumerate(lines):\n",
    "    doc = nlp(sentence.strip())  # Process with SpaCy\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in relevant_entities:\n",
    "            spacy_entities.append((idx, ent.start_char, ent.end_char, ent.label_))\n",
    "            spacy_entity_counter[ent.label_] += 1\n",
    "\n",
    "# Convert SpaCy counts to DataFrame for display\n",
    "spacy_df = pd.DataFrame(spacy_entity_counter.items(), columns=[\"Tag\", \"Count\"])\n",
    "spacy_df = spacy_df.sort_values(by=\"Tag\")\n",
    "\n",
    "print(\"\\nSpaCy Annotation Counts:\")\n",
    "print(spacy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd74af7e-9cec-459f-b401-188ddaddd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "      Tag   TP  FP  FN  Precision  Recall  F1 Score\n",
      "0  PERSON   81  18  15      0.818   0.844     0.831\n",
      "1    NORP   85  20  11      0.810   0.885     0.846\n",
      "2     ORG   81  32  21      0.717   0.794     0.753\n",
      "3     GPE  177  38  23      0.823   0.885     0.853\n",
      "4     LOC   26   7  38      0.788   0.406     0.536\n",
      "5    DATE  137  36   3      0.792   0.979     0.875\n",
      "6   MONEY   16   1   9      0.941   0.640     0.762\n"
     ]
    }
   ],
   "source": [
    "### PART 3: ALIGN AND EVALUATE ###\n",
    "def match_entities(manual_entities, spacy_entities):\n",
    "    tp = Counter()\n",
    "    fp = Counter()\n",
    "    fn = Counter()\n",
    "\n",
    "    matched_spacy = set()\n",
    "\n",
    "    # True Positives (TP) and False Negatives (FN)\n",
    "    for m_idx, m_start, m_end, m_label in manual_entities:\n",
    "        match_found = False\n",
    "        for s_idx, s_start, s_end, s_label in spacy_entities:\n",
    "            if (\n",
    "                m_idx == s_idx\n",
    "                and m_label == s_label\n",
    "                and not (s_end <= m_start or s_start >= m_end)  # Overlapping entities\n",
    "            ):\n",
    "                tp[m_label] += 1\n",
    "                matched_spacy.add((s_idx, s_start, s_end, s_label))\n",
    "                match_found = True\n",
    "                break\n",
    "        if not match_found:\n",
    "            fn[m_label] += 1\n",
    "\n",
    "    # False Positives (FP)\n",
    "    for s_idx, s_start, s_end, s_label in spacy_entities:\n",
    "        if (s_idx, s_start, s_end, s_label) not in matched_spacy:\n",
    "            fp[s_label] += 1\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "# Compute metrics\n",
    "tp, fp, fn = match_entities(manual_entities, spacy_entities)\n",
    "\n",
    "### PART 4: GENERATE CLASSIFICATION REPORT ###\n",
    "evaluation_data = []\n",
    "for tag in relevant_entities:\n",
    "    # Calculate Precision, Recall, and F1 Score\n",
    "    precision = tp[tag] / (tp[tag] + fp[tag]) if (tp[tag] + fp[tag]) > 0 else 0\n",
    "    recall = tp[tag] / (tp[tag] + fn[tag]) if (tp[tag] + fn[tag]) > 0 else 0\n",
    "    f1_score = (\n",
    "        2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Add data for each tag\n",
    "    evaluation_data.append({\n",
    "        \"Tag\": tag,\n",
    "        \"TP\": tp[tag],\n",
    "        \"FP\": fp[tag],\n",
    "        \"FN\": fn[tag],\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"F1 Score\": round(f1_score, 3),\n",
    "    })\n",
    "\n",
    "# Convert evaluation results to DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_data)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7e54d-6d82-4764-85cc-0c8e87a3857f",
   "metadata": {},
   "source": [
    "<h3>Performance analysis of SpaCy’s NER model:</h3>\r",
    "<ol>\n",
    "    <li> Precision: A Precision score is high, it indicates that SpaCy’s model predicts few false positives.\n",
    "What if Precision is low, it means SpaCy is incorrectly labeling entities where there are none.\n",
    "“MONEY” tag has high Precision. So, there are very less False Positives predicted by SpaCy.\n",
    "</li>\n",
    "    <li>Recall: A high recall score indicates SpaCy was able to correctly identify most of entities\n",
    "present in the manually annotated data. A low recall indicates that SpaCy missed many entities.\n",
    "“LOC” tag has low Recall. So, the SpaCy did not predict many LOC entities.\n",
    "</li>\n",
    "<li>F1 Score: It reflects how well SpaCy’s model is performing in terms of both recognizing\n",
    "entities correctly and covering all of them.\n",
    "\n",
    "</li>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "<h4>My SpaCy model well performed on annotating “DATE” tag which has high F1 Score.</h4>\n",
    "\n",
    "F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cf2a8-106c-4c95-9cd0-5336b29ff05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
